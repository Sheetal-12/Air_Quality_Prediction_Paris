{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54fe5d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (40991, 213) Test (504, 208)\n",
      "Found 28 lag bases (examples): ['apparent_temperature', 'cloud_cover', 'cloud_cover_high', 'cloud_cover_low', 'cloud_cover_mid', 'dew_point_2m']\n",
      "Found 110 roll cols (examples): ['valeur_NO2_roll_mean_6', 'valeur_NO2_roll_std_6', 'valeur_NO2_roll_mean_24', 'valeur_NO2_roll_std_24', 'valeur_CO_roll_mean_6', 'valeur_CO_roll_std_6']\n",
      "Found 2 other static cols (samples): ['NO2_lag1_for_O3', 'PM10_lag1_for_PM25']\n",
      "Planned flat feature count: 206\n",
      "Samples after dropna: 40991\n",
      "Seq shapes: (34842, 3, 160) (6149, 3, 160)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,544</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout1d_1             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout1D</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">148,992</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">325</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m160\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │        \u001b[38;5;34m20,544\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout1d_1             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSpatialDropout1D\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m148,992\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │           \u001b[38;5;34m325\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">211,269</span> (825.27 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m211,269\u001b[0m (825.27 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">211,141</span> (824.77 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m211,141\u001b[0m (824.77 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> (512.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m128\u001b[0m (512.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 23ms/step - loss: 0.3580 - mae: 0.4045 - val_loss: 0.2026 - val_mae: 0.2753 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - loss: 0.2481 - mae: 0.3284 - val_loss: 0.1821 - val_mae: 0.2534 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - loss: 0.2206 - mae: 0.3059 - val_loss: 0.1710 - val_mae: 0.2397 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - loss: 0.2045 - mae: 0.2943 - val_loss: 0.1635 - val_mae: 0.2310 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - loss: 0.1965 - mae: 0.2869 - val_loss: 0.1553 - val_mae: 0.2251 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - loss: 0.1858 - mae: 0.2801 - val_loss: 0.1543 - val_mae: 0.2269 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - loss: 0.1804 - mae: 0.2747 - val_loss: 0.1524 - val_mae: 0.2252 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 0.1766 - mae: 0.2712 - val_loss: 0.1536 - val_mae: 0.2288 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - loss: 0.1706 - mae: 0.2673 - val_loss: 0.1483 - val_mae: 0.2245 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - loss: 0.1662 - mae: 0.2638 - val_loss: 0.1468 - val_mae: 0.2332 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - loss: 0.1625 - mae: 0.2617 - val_loss: 0.1519 - val_mae: 0.2397 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 25ms/step - loss: 0.1603 - mae: 0.2590 - val_loss: 0.1506 - val_mae: 0.2389 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 36ms/step - loss: 0.1577 - mae: 0.2565 - val_loss: 0.1542 - val_mae: 0.2445 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 0.1538 - mae: 0.2543 - val_loss: 0.1519 - val_mae: 0.2434 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m271/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1471 - mae: 0.2525\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 0.1522 - mae: 0.2535 - val_loss: 0.1522 - val_mae: 0.2477 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 0.1422 - mae: 0.2447 - val_loss: 0.1479 - val_mae: 0.2432 - learning_rate: 5.0000e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - loss: 0.1386 - mae: 0.2434 - val_loss: 0.1470 - val_mae: 0.2443 - learning_rate: 5.0000e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - loss: 0.1378 - mae: 0.2413 - val_loss: 0.1498 - val_mae: 0.2458 - learning_rate: 5.0000e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - loss: 0.1359 - mae: 0.2409 - val_loss: 0.1503 - val_mae: 0.2489 - learning_rate: 5.0000e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - loss: 0.1332 - mae: 0.2407 - val_loss: 0.1430 - val_mae: 0.2440 - learning_rate: 5.0000e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - loss: 0.1322 - mae: 0.2393 - val_loss: 0.1500 - val_mae: 0.2474 - learning_rate: 5.0000e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - loss: 0.1279 - mae: 0.2378 - val_loss: 0.1451 - val_mae: 0.2474 - learning_rate: 5.0000e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - loss: 0.1268 - mae: 0.2376 - val_loss: 0.1417 - val_mae: 0.2457 - learning_rate: 5.0000e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - loss: 0.1278 - mae: 0.2374 - val_loss: 0.1474 - val_mae: 0.2495 - learning_rate: 5.0000e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - loss: 0.1274 - mae: 0.2372 - val_loss: 0.1507 - val_mae: 0.2535 - learning_rate: 5.0000e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - loss: 0.1240 - mae: 0.2358 - val_loss: 0.1471 - val_mae: 0.2485 - learning_rate: 5.0000e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 23ms/step - loss: 0.1248 - mae: 0.2351 - val_loss: 0.1497 - val_mae: 0.2477 - learning_rate: 5.0000e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m269/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1234 - mae: 0.2338\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - loss: 0.1234 - mae: 0.2343 - val_loss: 0.1535 - val_mae: 0.2540 - learning_rate: 5.0000e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - loss: 0.1178 - mae: 0.2313 - val_loss: 0.1524 - val_mae: 0.2510 - learning_rate: 2.5000e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - loss: 0.1165 - mae: 0.2296 - val_loss: 0.1463 - val_mae: 0.2530 - learning_rate: 2.5000e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - loss: 0.1142 - mae: 0.2281 - val_loss: 0.1463 - val_mae: 0.2516 - learning_rate: 2.5000e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - loss: 0.1139 - mae: 0.2277 - val_loss: 0.1519 - val_mae: 0.2573 - learning_rate: 2.5000e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m272/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.1147 - mae: 0.2273\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 23ms/step - loss: 0.1162 - mae: 0.2274 - val_loss: 0.1479 - val_mae: 0.2511 - learning_rate: 2.5000e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - loss: 0.1133 - mae: 0.2260 - val_loss: 0.1463 - val_mae: 0.2517 - learning_rate: 1.2500e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - loss: 0.1090 - mae: 0.2244 - val_loss: 0.1457 - val_mae: 0.2504 - learning_rate: 1.2500e-04\n",
      "Epoch 35: early stopping\n",
      "Restoring model weights from the end of the best epoch: 23.\n",
      "valeur_NO2: MAE = 4.1226\n",
      "valeur_CO: MAE = 0.0262\n",
      "valeur_O3: MAE = 6.5255\n",
      "valeur_PM10: MAE = 2.5195\n",
      "valeur_PM25: MAE = 1.8417\n",
      "Avg MAE: 3.0071050656938754\n"
     ]
    }
   ],
   "source": [
    "# GRU + 1D-CNN temporal model built from your existing lag/roll features.\n",
    "# Assumptions:\n",
    "# - LAGS = [6,12,24] correspond to columns with suffixes '_lag_6','_lag_12','_lag_24'\n",
    "# - roll_mean / roll_std features are kept and broadcasted to each timestep\n",
    "# - Targets: same as your original TARGETS\n",
    "# - Paths: same as your original script (../data/...)\n",
    "# - This trains a model from scratch (replace hyperparams if needed)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers, callbacks\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Repro\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# ====== CONFIG ======\n",
    "TARGETS = ['valeur_NO2', 'valeur_CO', 'valeur_O3', 'valeur_PM10', 'valeur_PM25']\n",
    "TEMPORAL_FEATURES = ['hour', 'is_day', 'hour_sin', 'hour_cos', 'dow', 'dow_sin', \n",
    "                     'dow_cos', 'is_holiday', 'is_weekend', 'lockdown_code']\n",
    "LAGS = [6, 12, 24]\n",
    "TRAIN_PATH = \"../data/train_features.csv\"\n",
    "TEST_PATH = \"../data/test_features_to_predict.csv\"\n",
    "\n",
    "# ====== LOAD ======\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "test_df = pd.read_csv(TEST_PATH)\n",
    "print(\"Train\", train_df.shape, \"Test\", test_df.shape)\n",
    "\n",
    "# ====== Identify lag bases and roll/static features ======\n",
    "cols = train_df.columns.tolist()\n",
    "\n",
    "# discover lag columns and their base names like 'valeur_NO2' (from 'valeur_NO2_lag_6')\n",
    "lag_cols = {lag: [c for c in cols if f\"_lag_{lag}\" in c] for lag in LAGS}\n",
    "# create set of bases that have at least one lag\n",
    "bases = set()\n",
    "for lag in LAGS:\n",
    "    for c in lag_cols[lag]:\n",
    "        base = c.replace(f\"_lag_{lag}\", \"\")\n",
    "        bases.add(base)\n",
    "bases = sorted(bases)\n",
    "\n",
    "# collect roll_* columns (roll_mean/_std) and other non-lag static features\n",
    "roll_cols = [c for c in cols if ('roll_mean' in c) or ('roll_std' in c)]\n",
    "# other features we will treat as static (e.g., the TEMPORAL_FEATURES are handled separately)\n",
    "static_cols = [c for c in cols if (c not in sum(lag_cols.values(), []) and c not in roll_cols + TEMPORAL_FEATURES + TARGETS + ['id','datetime'])]\n",
    "\n",
    "print(f\"Found {len(bases)} lag bases (examples): {bases[:6]}\")\n",
    "print(f\"Found {len(roll_cols)} roll cols (examples): {roll_cols[:6]}\")\n",
    "print(f\"Found {len(static_cols)} other static cols (samples): {static_cols[:6]}\")\n",
    "\n",
    "# ====== Build feature ordering for flat representation (same as before) ======\n",
    "# We'll build a flattened feature vector for each sample like earlier (so scaling is straightforward),\n",
    "# then reshape into (samples, timesteps=3, channels) for the temporal model.\n",
    "# Flatten order: for each base in bases -> [lag_6, lag_12, lag_24] (fill missing with most recent available)\n",
    "flat_feature_names = []\n",
    "# include lag sequence for each base\n",
    "for base in bases:\n",
    "    for lag in LAGS:\n",
    "        colname = f\"{base}_lag_{lag}\"\n",
    "        if colname in cols:\n",
    "            flat_feature_names.append(colname)\n",
    "        else:\n",
    "            # fallback: try to use a related column (e.g. if not present, use 0 placeholder)\n",
    "            flat_feature_names.append(None)  # mark missing, will handle later\n",
    "\n",
    "# append roll features and other static features (kept as additional flat features)\n",
    "flat_feature_names += roll_cols + static_cols + TEMPORAL_FEATURES\n",
    "# remove duplicates and None will be handled when building arrays\n",
    "# compute number of expected features\n",
    "print(\"Planned flat feature count:\", len(flat_feature_names))\n",
    "\n",
    "# ====== Prepare training data (drop rows with NaNs as you did) ======\n",
    "# Build DataFrame for required flat features (replace missing cols with NaN so dropna removes them)\n",
    "flat_df = pd.DataFrame()\n",
    "for name in flat_feature_names:\n",
    "    if name is None:\n",
    "        flat_df[name or \"MISSING\"] = np.nan  # will be dropped by dropna\n",
    "    else:\n",
    "        flat_df[name] = train_df.get(name, np.nan)\n",
    "\n",
    "# Combine with temporal features\n",
    "for tcol in TEMPORAL_FEATURES:\n",
    "    if tcol in train_df.columns:\n",
    "        flat_df[tcol] = train_df[tcol]\n",
    "    else:\n",
    "        flat_df[tcol] = np.nan\n",
    "\n",
    "# Add targets\n",
    "for tgt in TARGETS:\n",
    "    flat_df[tgt] = train_df[tgt]\n",
    "\n",
    "# drop rows with NaNs in any required flat feature or target (same approach)\n",
    "clean = flat_df.dropna()\n",
    "print(\"Samples after dropna:\", clean.shape[0])\n",
    "\n",
    "# Extract X_flat and Y\n",
    "X_flat = clean[[c for c in flat_feature_names if c is not None] + TEMPORAL_FEATURES].values\n",
    "Y = clean[TARGETS].values\n",
    "\n",
    "# ====== Train/Val split like before ======\n",
    "split_idx = int(0.85 * len(X_flat))\n",
    "X_train_flat = X_flat[:split_idx]\n",
    "X_val_flat = X_flat[split_idx:]\n",
    "Y_train = Y[:split_idx]\n",
    "Y_val = Y[split_idx:]\n",
    "\n",
    "# ====== Scale features & targets (same logic as yours) ======\n",
    "feature_scaler = StandardScaler()\n",
    "X_train_scaled_flat = feature_scaler.fit_transform(X_train_flat)\n",
    "X_val_scaled_flat = feature_scaler.transform(X_val_flat)\n",
    "\n",
    "# target scalers per pollutant\n",
    "target_scalers = {}\n",
    "Y_train_scaled = np.zeros_like(Y_train, dtype=np.float32)\n",
    "Y_val_scaled = np.zeros_like(Y_val, dtype=np.float32)\n",
    "for i, tgt in enumerate(TARGETS):\n",
    "    s = StandardScaler()\n",
    "    Y_train_scaled[:, i] = s.fit_transform(Y_train[:, i:i+1]).ravel()\n",
    "    Y_val_scaled[:, i] = s.transform(Y_val[:, i:i+1]).ravel()\n",
    "    target_scalers[tgt] = s\n",
    "\n",
    "# ====== Reshape flattened scaled features into sequence (timesteps=3) ======\n",
    "# To do this we need to know how many lag channels exist (len(bases)*len(LAGS) possibly)\n",
    "n_bases = len(bases)\n",
    "timesteps = len(LAGS)  # 3\n",
    "# number of lag columns present in flat_feature_names equals n_bases * timesteps (with possible missing)\n",
    "lag_flat_count = n_bases * timesteps\n",
    "\n",
    "# We'll assume the flat ordering started with lag sequence for each base (see above)\n",
    "# Build function to convert scaled flat vector -> (timesteps, channels)\n",
    "def flat_to_seq(X_scaled_flat):\n",
    "    n_samples = X_scaled_flat.shape[0]\n",
    "    # first part corresponds to lag sequence region\n",
    "    lag_region = X_scaled_flat[:, :lag_flat_count]\n",
    "    # reshape into (samples, n_bases, timesteps) then transpose to (samples, timesteps, n_bases)\n",
    "    lag_region = lag_region.reshape(n_samples, n_bases, timesteps)\n",
    "    seq = np.transpose(lag_region, (0, 2, 1))  # (samples, timesteps, channels_per_timestep=n_bases)\n",
    "    # remaining features (rolls, statics, temporal) appended after lag region in flat vector\n",
    "    remaining = X_scaled_flat[:, lag_flat_count:]\n",
    "    # broadcast remaining features into each timestep (so each timestep has same additional features)\n",
    "    rem_per_timestep = np.repeat(remaining[:, np.newaxis, :], timesteps, axis=1)  # shape (samples, timesteps, rem_dim)\n",
    "    seq_full = np.concatenate([seq, rem_per_timestep], axis=2)  # (samples, timesteps, channels)\n",
    "    return seq_full\n",
    "\n",
    "X_train_seq = flat_to_seq(X_train_scaled_flat)\n",
    "X_val_seq = flat_to_seq(X_val_scaled_flat)\n",
    "print(\"Seq shapes:\", X_train_seq.shape, X_val_seq.shape)  # (samples, timesteps, channels)\n",
    "\n",
    "# ====== Build model: Conv1D -> BiGRU -> Dense (multi-output) ======\n",
    "def build_temporal_model(input_shape, n_targets=len(TARGETS)):\n",
    "    inp = layers.Input(shape=input_shape)  # (timesteps, channels)\n",
    "    x = layers.Conv1D(64, kernel_size=2, padding='causal', activation='relu')(inp)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.SpatialDropout1D(0.2)(x)\n",
    "    x = layers.Bidirectional(layers.GRU(128, return_sequences=False, dropout=0.2))(x)\n",
    "    x = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    out = layers.Dense(n_targets)(x)\n",
    "    model = models.Model(inp, out)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "model = build_temporal_model(X_train_seq.shape[1:])\n",
    "model.summary()\n",
    "\n",
    "# ====== Train ======\n",
    "es = callbacks.EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True, verbose=1)\n",
    "rlr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_seq, Y_train_scaled,\n",
    "    validation_data=(X_val_seq, Y_val_scaled),\n",
    "    epochs=100,\n",
    "    batch_size=128,\n",
    "    callbacks=[es, rlr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ====== Validation metrics in original scale ======\n",
    "Y_val_pred_scaled = model.predict(X_val_seq, verbose=0)\n",
    "mae_per_pollutant = []\n",
    "for i, tgt in enumerate(TARGETS):\n",
    "    y_pred_unscaled = target_scalers[tgt].inverse_transform(Y_val_pred_scaled[:, i:i+1]).ravel()\n",
    "    mae = mean_absolute_error(Y_val[:, i], y_pred_unscaled)\n",
    "    mae_per_pollutant.append(mae)\n",
    "    print(f\"{tgt}: MAE = {mae:.4f}\")\n",
    "print(\"Avg MAE:\", np.mean(mae_per_pollutant))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ae565c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing test data with rolling lag features...\n",
      "feature_cols length: 216\n",
      "Using last 24 rows of train for feature averaging\n",
      "Test features shape: (504, 216)\n",
      "Expected features: 216\n",
      "X_test_seq shape (for model): (504, 3, 160)\n",
      "\n",
      "Predicting on test set...\n",
      "✅ Submission saved to submission.csv\n",
      "\n",
      "Submission preview:\n",
      "              id  valeur_NO2  valeur_CO  valeur_O3  valeur_PM10  valeur_PM25\n",
      "0  2024-09-03 23   18.036446   0.175036  42.576515    10.783887     6.141726\n",
      "1  2024-09-04 00   19.455297   0.177276  40.633728    12.067898     6.740830\n",
      "2  2024-09-04 01   19.896864   0.175878  39.447834    11.909089     6.649740\n",
      "3  2024-09-04 02   20.738203   0.175373  38.268314    11.807177     6.507667\n",
      "4  2024-09-04 03   21.816051   0.176506  37.283451    11.743945     6.340149\n",
      "\n",
      "Submission shape: (504, 6)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================ \n",
    "# PREPARE TEST DATA - ROLLING AVERAGE APPROACH \n",
    "# ============================================================================ \n",
    "print(\"\\nPreparing test data with rolling lag features...\")\n",
    "\n",
    "# Get last 24 rows of train to use for averaging (more realistic than single row)\n",
    "last_24_rows = train_df.iloc[-24:].copy()\n",
    "\n",
    "# Get the exact feature columns used during training\n",
    "feature_cols = [c for c in flat_feature_names if c is not None] + TEMPORAL_FEATURES\n",
    "n_features = len(feature_cols)\n",
    "\n",
    "print(f\"feature_cols length: {n_features}\")\n",
    "print(f\"Using last 24 rows of train for feature averaging\")\n",
    "\n",
    "# Build test features: average features from last 24 train rows, update temporal from test_df\n",
    "test_features_list = []\n",
    "\n",
    "for idx in range(len(test_df)):\n",
    "    test_row = test_df.iloc[idx].copy()\n",
    "    \n",
    "    # Average each feature across last 24 train rows\n",
    "    base_features = []\n",
    "    for col in feature_cols:\n",
    "        if col in last_24_rows.columns:\n",
    "            # For temporal features, we'll override later; for others, take mean\n",
    "            if col not in TEMPORAL_FEATURES:\n",
    "                feat_val = float(last_24_rows[col].mean())\n",
    "            else:\n",
    "                feat_val = float(last_24_rows[col].iloc[-1])  # Use last value for temporal\n",
    "            base_features.append(feat_val)\n",
    "        else:\n",
    "            base_features.append(0.0)\n",
    "    \n",
    "    base_features = np.array(base_features)\n",
    "    \n",
    "    # Update temporal features from test_df (these change for each test row)\n",
    "    for i, col in enumerate(feature_cols):\n",
    "        if col in TEMPORAL_FEATURES and col in test_row.index:\n",
    "            base_features[i] = float(test_row[col])\n",
    "    \n",
    "    test_features_list.append(base_features)\n",
    "\n",
    "X_test = np.array(test_features_list)\n",
    "print(f\"Test features shape: {X_test.shape}\")\n",
    "print(f\"Expected features: {n_features}\")\n",
    "\n",
    "# Scale test features using the scaler fitted on training data\n",
    "X_test_scaled = feature_scaler.transform(X_test)\n",
    "\n",
    "# Reshape into sequences for GRU+CNN model\n",
    "X_test_seq = flat_to_seq(X_test_scaled)\n",
    "print(f\"X_test_seq shape (for model): {X_test_seq.shape}\")\n",
    "\n",
    "# ============================================================================ \n",
    "# PREDICT\n",
    "# ============================================================================ \n",
    "print(\"\\nPredicting on test set...\")\n",
    "Y_test_pred_scaled = model.predict(X_test_seq, verbose=0)\n",
    "\n",
    "# Inverse scale predictions per pollutant\n",
    "Y_test_pred = np.zeros_like(Y_test_pred_scaled)\n",
    "for i, target in enumerate(TARGETS):\n",
    "    Y_test_pred[:, i] = target_scalers[target].inverse_transform(Y_test_pred_scaled[:, i:i+1]).ravel()\n",
    "\n",
    "# ============================================================================ \n",
    "# CREATE SUBMISSION\n",
    "# ============================================================================ \n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = test_df['id'].values\n",
    "for i, target in enumerate(TARGETS):\n",
    "    submission[target] = Y_test_pred[:, i]\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"✅ Submission saved to submission.csv\")\n",
    "print(f\"\\nSubmission preview:\\n{submission.head()}\")\n",
    "print(f\"\\nSubmission shape: {submission.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "502f4d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing test data with rolling lag features...\n",
      "Number of features for test: 206\n",
      "Test features shape: (504, 206)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 206 features, but StandardScaler is expecting 216 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTest features shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_test.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Scale features using the scaler fitted on training data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m X_test_scaled = \u001b[43mfeature_scaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Reshape into sequences for GRU+CNN model\u001b[39;00m\n\u001b[32m     48\u001b[39m X_test_seq = flat_to_seq(X_test_scaled)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\preprocessing\\_data.py:1075\u001b[39m, in \u001b[36mStandardScaler.transform\u001b[39m\u001b[34m(self, X, copy)\u001b[39m\n\u001b[32m   1072\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1074\u001b[39m copy = copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy\n\u001b[32m-> \u001b[39m\u001b[32m1075\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1076\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1077\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1078\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1079\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1080\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1081\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1082\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1083\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1084\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1086\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sparse.issparse(X):\n\u001b[32m   1087\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.with_mean:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:2975\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2972\u001b[39m     out = X, y\n\u001b[32m   2974\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m2975\u001b[39m     \u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2977\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:2839\u001b[39m, in \u001b[36m_check_n_features\u001b[39m\u001b[34m(estimator, X, reset)\u001b[39m\n\u001b[32m   2836\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   2838\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_features != estimator.n_features_in_:\n\u001b[32m-> \u001b[39m\u001b[32m2839\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2840\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2841\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator.n_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features as input.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2842\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: X has 206 features, but StandardScaler is expecting 216 features as input."
     ]
    }
   ],
   "source": [
    "# ============================================================================ \n",
    "# PREPARE TEST DATA - ROLLING AVERAGE APPROACH \n",
    "# ============================================================================ \n",
    "print(\"\\nPreparing test data with rolling lag features...\")\n",
    "\n",
    "# Get last 24 rows of train to use for averaging (more realistic than single row)\n",
    "last_24_rows = train_df.iloc[-24:].copy()\n",
    "\n",
    "# Only take temporal features from test_df that are in our original TEMPORAL_FEATURES list\n",
    "temporal_cols_in_test = [c for c in TEMPORAL_FEATURES if c in test_df.columns]\n",
    "\n",
    "# Remaining features are all used in training except temporal\n",
    "remaining_cols = [c for c in used_flat_names if c not in TEMPORAL_FEATURES]\n",
    "\n",
    "# Combine in order for building test vectors\n",
    "feature_cols = remaining_cols + temporal_cols_in_test\n",
    "n_features = len(feature_cols)\n",
    "print(f\"Number of features for test: {n_features}\")\n",
    "\n",
    "# Build test features\n",
    "test_features_list = []\n",
    "for idx in range(len(test_df)):\n",
    "    test_row = test_df.iloc[idx].copy()\n",
    "    \n",
    "    base_features = []\n",
    "    for col in feature_cols:\n",
    "        if col in temporal_cols_in_test:\n",
    "            # placeholder from last train row; will override with test value\n",
    "            base_features.append(float(last_24_rows[col].iloc[-1]) if col in last_24_rows else 0.0)\n",
    "        else:\n",
    "            # rolling mean from last 24 train rows\n",
    "            base_features.append(float(last_24_rows[col].mean()) if col in last_24_rows else 0.0)\n",
    "    \n",
    "    # Override temporal features with actual test row values\n",
    "    for i, col in enumerate(feature_cols):\n",
    "        if col in temporal_cols_in_test:\n",
    "            base_features[i] = float(test_row[col])\n",
    "    \n",
    "    test_features_list.append(base_features)\n",
    "\n",
    "X_test = np.array(test_features_list)\n",
    "print(f\"Test features shape: {X_test.shape}\")\n",
    "\n",
    "# Scale features using the scaler fitted on training data\n",
    "X_test_scaled = feature_scaler.transform(X_test)\n",
    "\n",
    "# Reshape into sequences for GRU+CNN model\n",
    "X_test_seq = flat_to_seq(X_test_scaled)\n",
    "print(f\"X_test_seq shape (for model): {X_test_seq.shape}\")\n",
    "\n",
    "# ============================================================================ \n",
    "# PREDICT\n",
    "# ============================================================================ \n",
    "print(\"\\nPredicting on test set...\")\n",
    "Y_test_pred_scaled = model.predict(X_test_seq, verbose=0)\n",
    "\n",
    "# Inverse scale predictions per pollutant\n",
    "Y_test_pred = np.zeros_like(Y_test_pred_scaled)\n",
    "for i, target in enumerate(TARGETS):\n",
    "    Y_test_pred[:, i] = target_scalers[target].inverse_transform(Y_test_pred_scaled[:, i:i+1]).ravel()\n",
    "\n",
    "# ============================================================================ \n",
    "# CREATE SUBMISSION\n",
    "# ============================================================================ \n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = test_df['id'].values\n",
    "for i, target in enumerate(TARGETS):\n",
    "    submission[target] = Y_test_pred[:, i]\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"✅ Submission saved to submission.csv\")\n",
    "print(f\"\\nSubmission preview:\\n{submission.head()}\")\n",
    "print(f\"\\nSubmission shape: {submission.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c707d042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Prepare test features exactly like your previous approach, then scale & reshape ======\n",
    "# Recreate your test-building logic (average last 24 rows of train_df for non-temporal features)\n",
    "last_24 = train_df.iloc[-24:].copy()\n",
    "feature_cols = [c for c in cols if ('_lag_' in c) or ('roll_' in c)] + TEMPORAL_FEATURES\n",
    "\n",
    "test_feats_list = []\n",
    "for idx in range(len(test_df)):\n",
    "    tr = test_df.iloc[idx]\n",
    "    base_features = []\n",
    "    for name in flat_feature_names:\n",
    "        # skip None markers\n",
    "        if name is None:\n",
    "            base_features.append(np.nan)\n",
    "            continue\n",
    "        if name in last_24.columns:\n",
    "            if name in TEMPORAL_FEATURES:\n",
    "                base_features.append(float(last_24[name].iloc[-1]))\n",
    "            else:\n",
    "                base_features.append(float(last_24[name].mean()))\n",
    "        else:\n",
    "            base_features.append(0.0)\n",
    "    # override temporal vars from test row\n",
    "    for i, name in enumerate(flat_feature_names):\n",
    "        if name in TEMPORAL_FEATURES and name in tr.index:\n",
    "            base_features[i] = float(tr[name])\n",
    "    test_feats_list.append(base_features)\n",
    "\n",
    "X_test_flat = np.array(test_feats_list)  # shape (504, n_flat)\n",
    "print(\"Raw X_test_flat shape:\", X_test_flat.shape)\n",
    "\n",
    "# Align columns used for scaling: we trained scalers on the subset (excluded None entries)\n",
    "# Build X_test array matching columns used in scaling: keep only non-None flat_feature entries + temporal (we used this earlier)\n",
    "used_flat_names = [c for c in flat_feature_names if c is not None] + TEMPORAL_FEATURES\n",
    "X_test_used = X_test_flat[:, :len(used_flat_names)]\n",
    "\n",
    "# scale then reshape\n",
    "X_test_scaled_flat = feature_scaler.transform(X_test_used)\n",
    "X_test_seq = flat_to_seq(X_test_scaled_flat)\n",
    "print(\"X_test_seq shape:\", X_test_seq.shape)\n",
    "\n",
    "# ====== Predict on test and inverse-scale predictions ======\n",
    "Y_test_pred_scaled = model.predict(X_test_seq, verbose=0)\n",
    "Y_test_pred = np.zeros_like(Y_test_pred_scaled)\n",
    "for i, tgt in enumerate(TARGETS):\n",
    "    Y_test_pred[:, i] = target_scalers[tgt].inverse_transform(Y_test_pred_scaled[:, i:i+1]).ravel()\n",
    "\n",
    "submission = pd.DataFrame({'id': test_df['id']})\n",
    "for i, tgt in enumerate(TARGETS):\n",
    "    submission[tgt] = Y_test_pred[:, i]\n",
    "submission.to_csv(\"gru_cnn_submission.csv\", index=False)\n",
    "print(\"Saved gru_cnn_submission.csv — shape:\", submission.shape)\n",
    "print(submission.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
