{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"traindata_with_lockdown.csv\"   # update if needed\n",
    "TEST_PATH  = \"testdata_with_lockdown.csv\"\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# LOAD\n",
    "# -----------------------------\n",
    "df_tr = pd.read_csv(TRAIN_PATH)\n",
    "df_te = pd.read_csv(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groups used: __grp__ (1 unique)\n",
      "X_train: (40991, 316) | Y_train: (40991, 5)\n",
      "X_test_pred: (504, 316) | meta_test: (504, 2)\n",
      "Num features: 316\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "TIME = \"datetime\"\n",
    "RAW_ID = \"id\"  # might be wrong in your data; we will auto-fix below\n",
    "\n",
    "TARGETS = [\"valeur_NO2\",\"valeur_CO\",\"valeur_O3\",\"valeur_PM10\",\"valeur_PM25\"]\n",
    "\n",
    "WEATHER = [\n",
    "    'temperature_2m','relative_humidity_2m','dew_point_2m','apparent_temperature',\n",
    "    'pressure_msl','wind_speed_10m','wind_direction_10m','precipitation',\n",
    "    'cloud_cover','cloud_cover_low','cloud_cover_mid','cloud_cover_high',\n",
    "    'shortwave_radiation','direct_radiation','diffuse_radiation','global_tilted_irradiance',\n",
    "    'wind_speed_80m','wind_speed_120m','wind_speed_180m',\n",
    "    'wind_direction_80m','wind_direction_120m','wind_direction_180m',\n",
    "    'wind_gusts_10m','vapour_pressure_deficit','cape','evapotranspiration',\n",
    "    'et0_fao_evapotranspiration','snowfall','rain','showers','weather_code',\n",
    "    'visibility','lockdown_code'\n",
    "]\n",
    "STATIC_KNOWN = [\"is_holiday\",\"is_weekend\",\"lockdown_code\"]\n",
    "\n",
    "LAGS  = [1, 6, 12, 24]\n",
    "ROLLS = [6, 24]   # hours\n",
    "\n",
    "# =========================\n",
    "# MAIN\n",
    "# =========================\n",
    "def build_dataset(df_tr: pd.DataFrame, df_te: pd.DataFrame):\n",
    "    # --- Tidy/union ---\n",
    "    df_tr = df_tr.copy()\n",
    "    df_te = df_te.copy()\n",
    "    df_tr[TIME] = pd.to_datetime(df_tr[TIME])\n",
    "    df_te[TIME] = pd.to_datetime(df_te[TIME])\n",
    "    df_tr[\"__split__\"] = \"train\"; df_te[\"__split__\"] = \"test\"\n",
    "    df = pd.concat([df_tr, df_te], ignore_index=True)\n",
    "    df = df.sort_values([TIME]).reset_index(drop=True)\n",
    "\n",
    "    # --- Choose grouping key ---\n",
    "    # If RAW_ID repeats, use it. Otherwise create a single group \"__grp__\" so lags/rolls work.\n",
    "    use_id = RAW_ID if RAW_ID in df.columns else None\n",
    "    if use_id is not None:\n",
    "        nunq = df[RAW_ID].nunique()\n",
    "        if nunq == len(df):  # all unique -> useless for groupby\n",
    "            use_id = None\n",
    "    if use_id is None:\n",
    "        df[\"__grp__\"] = \"all\"\n",
    "        GROUP = \"__grp__\"\n",
    "    else:\n",
    "        GROUP = RAW_ID\n",
    "        # make sure it's clean type\n",
    "        df[GROUP] = df[GROUP].astype(str).str.strip()\n",
    "\n",
    "    # --- Which weather cols exist ---\n",
    "    weather_cols = [c for c in WEATHER if c in df.columns]\n",
    "\n",
    "    # --- Mark future test horizon (first row per group where ANY target is NaN) ---\n",
    "    te = df[df[\"__split__\"] == \"test\"]\n",
    "    if len(te):\n",
    "        # compute per-group first future timestamp\n",
    "        tmp = te.copy()\n",
    "        tmp[\"_nan_any\"] = tmp[TARGETS].isna().any(axis=1)\n",
    "        first_future = (\n",
    "            tmp[tmp[\"_nan_any\"]]\n",
    "            .groupby(GROUP, as_index=True)[TIME]\n",
    "            .min()\n",
    "        )\n",
    "        df = df.join(first_future.rename(\"__future_start__\"), on=GROUP)\n",
    "        df[\"__is_future_test__\"] = (df[\"__split__\"]==\"test\") & df[\"__future_start__\"].notna() & (df[TIME] >= df[\"__future_start__\"])\n",
    "        df.drop(columns=\"__future_start__\", inplace=True)\n",
    "    else:\n",
    "        df[\"__is_future_test__\"] = False\n",
    "\n",
    "    # --- Time features ---\n",
    "    df[\"hour\"] = df[TIME].dt.hour\n",
    "    df[\"is_day\"] = ((df[\"hour\"] >= 6) & (df[\"hour\"] < 18)).astype(int)\n",
    "    df[\"hour_sin\"] = np.sin(2*np.pi*df[\"hour\"]/24)\n",
    "    df[\"hour_cos\"] = np.cos(2*np.pi*df[\"hour\"]/24)\n",
    "    df[\"dow\"] = df[TIME].dt.dayofweek\n",
    "    df[\"dow_sin\"] = np.sin(2*np.pi*df[\"dow\"]/7)\n",
    "    df[\"dow_cos\"] = np.cos(2*np.pi*df[\"dow\"]/7)\n",
    "\n",
    "    # --- Mask future test weather (rule) ---\n",
    "    if weather_cols:\n",
    "        df.loc[df[\"__is_future_test__\"], weather_cols] = np.nan\n",
    "\n",
    "    # --- Build features into a dict (avoid fragmentation) ---\n",
    "    feats = {}\n",
    "\n",
    "    # Helper: add series to dict safely\n",
    "    def add_feat(name, series):\n",
    "        feats[name] = series\n",
    "\n",
    "    # POLLUTANT lags/rollings (past only)\n",
    "    for col in TARGETS:\n",
    "        g = df.groupby(GROUP)[col]\n",
    "        for l in LAGS:\n",
    "            add_feat(f\"{col}_lag_{l}\", g.shift(l))\n",
    "        s = g.shift(1)  # past-only base\n",
    "        for w in ROLLS:\n",
    "            add_feat(f\"{col}_roll_mean_{w}\", s.groupby(df[GROUP]).rolling(w, min_periods=1).mean().reset_index(level=0, drop=True))\n",
    "            add_feat(f\"{col}_roll_std_{w}\",  s.groupby(df[GROUP]).rolling(w, min_periods=2).std().reset_index(level=0, drop=True))\n",
    "\n",
    "    # Cross-lags (examples)\n",
    "    add_feat(\"NO2_lag1_for_O3\", df.groupby(GROUP)[\"valeur_NO2\"].shift(1))\n",
    "    add_feat(\"PM10_lag1_for_PM25\", df.groupby(GROUP)[\"valeur_PM10\"].shift(1))\n",
    "\n",
    "    # WEATHER lags/rollings (past only, after masking)\n",
    "    for col in weather_cols:\n",
    "        g = df.groupby(GROUP)[col]\n",
    "        for l in LAGS:\n",
    "            add_feat(f\"{col}_lag_{l}\", g.shift(l))\n",
    "        s = g.shift(1)\n",
    "        for w in ROLLS:\n",
    "            add_feat(f\"{col}_roll_mean_{w}\", s.groupby(df[GROUP]).rolling(w, min_periods=1).mean().reset_index(level=0, drop=True))\n",
    "            # std only if numeric\n",
    "            if pd.api.types.is_numeric_dtype(df[col]):\n",
    "                add_feat(f\"{col}_roll_std_{w}\",  s.groupby(df[GROUP]).rolling(w, min_periods=2).std().reset_index(level=0, drop=True))\n",
    "\n",
    "    # Concatenate all features at once (fast, no fragmentation)\n",
    "    feat_df = pd.DataFrame(feats, index=df.index)\n",
    "\n",
    "    # --- Assemble full X with calendar/static features ---\n",
    "    base_cols = [\"hour\",\"is_day\",\"hour_sin\",\"hour_cos\",\"dow\",\"dow_sin\",\"dow_cos\"] + [c for c in STATIC_KNOWN if c in df.columns]\n",
    "    X_all = pd.concat([feat_df, df[base_cols]], axis=1)\n",
    "\n",
    "    # --- Impute features (ffill per group → train medians; std→0) ---\n",
    "    X_all = X_all.groupby(df[GROUP]).ffill()\n",
    "\n",
    "    train_meds = X_all[df[\"__split__\"]==\"train\"].median(numeric_only=True)\n",
    "    X_all = X_all.fillna(train_meds)\n",
    "\n",
    "    std_cols = [c for c in X_all.columns if \"_roll_std_\" in c]\n",
    "    if std_cols:\n",
    "        X_all[std_cols] = X_all[std_cols].fillna(0.0)\n",
    "\n",
    "    # --- Outputs ---\n",
    "    feat_cols = list(X_all.columns)\n",
    "    meta = df[[GROUP, TIME, \"__split__\", \"__is_future_test__\"]].copy()\n",
    "    Y_all = df[TARGETS].copy()\n",
    "\n",
    "    # Train\n",
    "    X_train = X_all[meta[\"__split__\"]==\"train\"].reset_index(drop=True)\n",
    "    Y_train = Y_all[meta[\"__split__\"]==\"train\"].reset_index(drop=True)\n",
    "\n",
    "    # Test rows to predict (future only)\n",
    "    mask_pred = (meta[\"__split__\"]==\"test\") & (meta[\"__is_future_test__\"])\n",
    "    X_test_pred = X_all[mask_pred].reset_index(drop=True)\n",
    "    meta_test = df.loc[mask_pred, [GROUP, TIME]].rename(columns={GROUP:\"id\"}).reset_index(drop=True)\n",
    "\n",
    "    print(f\"Groups used: {GROUP} ({df[GROUP].nunique()} unique)\")\n",
    "    print(\"X_train:\", X_train.shape, \"| Y_train:\", Y_train.shape)\n",
    "    print(\"X_test_pred:\", X_test_pred.shape, \"| meta_test:\", meta_test.shape)\n",
    "    print(\"Num features:\", len(feat_cols))\n",
    "\n",
    "    return X_train, Y_train, X_test_pred, meta_test, feat_cols\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test_pred, meta_test, feat_cols = build_dataset(df_tr, df_te)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nandana_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
